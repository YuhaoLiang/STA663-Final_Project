{
 "metadata": {
  "name": "",
  "signature": "sha256:bc42920d24fa1f664a09867b129e1b73aae5fb4c4525ddf327f8fd0b528e312c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Outlines of Scalable K-means||"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Background"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So far, k-means remains one of the most popular data processing algorithms. However, general k-means algorithm is not a good clustering algorithm in terms of efficiency and quality, which means it needs a long time to converge when the data set is large and it may just converge to the local optimum. As to quality, the recently proposed k-means++ initialization algorithm improves this, obtaining an initial set of centers that is provably close to the optimum solution. Now, there is a new algorithm, k-means||, obtaining a nearly optimal solution, based on k-means++, after a logarithmic number of passes, and then show that in practice a constant number of passes suffices, which also largely improves efficiency."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Basically, k-means|| is based on k-means++ and the largest difference between these two algorithm is the initialization part of the algorithm. Since the initialization of k-means++ is deterministic (the previous choices that determine which points are away in the current solution), it is nearly impossible to use parallel computation to improve efficiency. Instead, k-means|| algorithm samples O(k) points in each round and repeat the process for approximately O(log n) rounds, which can implement parallel computation."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Algorithm"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Pseudo-code\n",
      "\n",
      "def dist(x,y):\n",
      "return(distance(x,y))\n",
      "\n",
      "def cost(C):\n",
      "return(sum(min(dist(data-C))))\n",
      "\n",
      "c = sample(data)\n",
      "phi = cost(c)\n",
      "\n",
      "for i in range(O(log(phi))):\n",
      "\\begin{equation}\n",
      "prob = \\frac{l*dist(data[i],c)^2}{\\phi(c)}\n",
      "\\end{equation}\n",
      "\\begin{equation}\n",
      "c' = sample(data,prob)\n",
      "\\end{equation}\n",
      "\\begin{equation}\n",
      "c = [c,c']\n",
      "\\end{equation}\n",
      "\n",
      "def num_close(c):\n",
      "minc = argmin([dist(z,y) for y in data for z in c],axis = 1)\n",
      "return [sum(minc == c) for z in c]\n",
      "\n",
      "kmeans(c,cluster_number = k)\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Draft of unit test"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "* Item cost function - non_negativity\n",
      "* Item cost function - if c has more points cost should be smaller\n",
      "* Item cost function - if c has all points cost should be 0\n",
      "* Item probability in sampling is non negative\n",
      "* Item sum of probability in sampling is l (oversampling factor)\n",
      "* Item point in C of probability in sampling is 0\n",
      "* Item find number of closet points function - non negative integer\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Optimization - parallel implementation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the initialization of K-means||, we can use MapReduce model of computation, especially for step 4 and step 7. As to step 4, when we sample each point in the data set in each iteration, we can assign each mapper to sample independently and combine the result. For step 7, we can also assign each mapper to calculate the number of points in data set closer to $c_i$ than any other  potential centers in $C$. In my code, I can use MapReduce model twice for step 7. Basically, I can assign each mapper to find the closest center for a data point independently and then assign each mapper to calculate how many data point is closer to the specific center than other centers independently."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}