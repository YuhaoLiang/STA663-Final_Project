{
 "metadata": {
  "name": "",
  "signature": "sha256:684e9214247f2dbb5580155d42016c0bedd3622d852c1bf8f50ce8016604d877"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Outlines of Scalable K-means||"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Background"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So far, k-means remains one of the most popular data processing algorithms. However, general k-means algorithm is not a good clustering algorithm in terms of efficiency and quality, which means it needs a long time to converge when the data set is large and it may just converge to the local optimum. As to quality, the recently proposed k-means++ initialization algorithm improves this, obtaining an initial set of centers that is provably close to the optimum solution. Now, there is a new algorithm, k-means||, obtaining a nearly optimal solution, based on k-means++, after a logarithmic number of passes, and then show that in practice a constant number of passes suffices, which also largely improves efficiency."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Basically, k-means|| is based on k-means++ and the largest difference between these two algorithm is the initialization part of the algorithm. Since the initialization of k-means++ is deterministic (the previous choices that determine which points are away in the current solution), it is nearly impossible to use parallel computation to improve efficiency. Instead, k-means|| algorithm samples O(k) points in each round and repeat the process for approximately O(log n) rounds, which can implement parallel computation."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Algorithm"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Pseudo-code\n",
      "\n",
      "def dist(x,y):\n",
      "return(distance(x,y))\n",
      "\n",
      "def cost(C):\n",
      "return(sum(min(dist(data-C))))\n",
      "\n",
      "c = sample(data)\n",
      "phi = cost(c)\n",
      "\n",
      "for i in range(O(log(phi))):\n",
      "\\begin{equation}\n",
      "prob = \\frac{l*dist(data[i],c)^2}{\\phi(c)}\n",
      "\\end{equation}\n",
      "\\begin{equation}\n",
      "c' = sample(data,prob)\n",
      "\\end{equation}\n",
      "\\begin{equation}\n",
      "c = [c,c']\n",
      "\\end{equation}\n",
      "\n",
      "def num_close(c):\n",
      "minc = argmin([dist(z,y) for y in data for z in c],axis = 1)\n",
      "return [sum(minc == c) for z in c]\n",
      "\n",
      "kmeans(c,cluster_number = k)\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Draft of unit test"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "* Item cost function - non_negativity\n",
      "* Item cost function - if c has more points cost should be smaller\n",
      "* Item cost function - if c has all points cost should be 0\n",
      "* Item probability in sampling is non negative\n",
      "* Item sum of probability in sampling is l (oversampling factor)\n",
      "* Item point in C of probability in sampling is 0\n",
      "* Item find number of closet points function - non negative integer\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}