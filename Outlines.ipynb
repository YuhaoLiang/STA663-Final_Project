{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlines of Scalable K-means++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, k-means still remains one of the most popular data processing algorithms. It is a widely used technique for statistical data analysis in many fields, such as machine learning, pattern recognition, image analysis and bioinformatics. However, general k-means algorithm with random initialization is not a good clustering algorithm in terms of efficiency and quality, which means it needs a long time to converge when the data set is large and it may just converge to the local optimum. In order to improve the quality, we need to improve the initialization part of the k-means algorithm first, selecting the right centers to do clustering. Recently people have proposed k-means++ initialization algorithm, obtaining the initial centers which can be provably close to the optimal solution, largely improves the quality of k-means algorithm but the problem of inefficiency is still unsolved. Now, there is a new algorithm, k-means||, obtaining a nearly optimal solution after a logarithmic number of passes.\n",
    "\n",
    "Basically, k-means|| is based on k-means++ and the largest difference between these two algorithm is the initialization part of the algorithm. Since the initialization of k-means++ is deterministic (the previous choices that affect which points are choosed in the current solution), it is nearly impossible to use parallelized computation to improve efficiency. Instead, k-means|| algorithm samples each point independently in each round and repeat the process for approximately O(log $\\phi$) rounds, which can be easily implemented by means of parallel computation. Besides, $\\phi$ is the cluster cost of initial randomly picked center, which can be viewed as sum of the distances between initial center and other points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo-code\n",
    "\n",
    "def dist(x,y):\n",
    "return(distance(x,y))\n",
    "\n",
    "def cost(C):\n",
    "return(sum(min(dist(data-C))))\n",
    "\n",
    "c = sample(data)\n",
    "phi = cost(c)\n",
    "\n",
    "for i in range(O(log(phi))):\n",
    "\\begin{equation}\n",
    "prob = \\frac{l*dist(data[i],c)^2}{\\phi(c)}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "c' = sample(data,prob)\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "c = [c,c']\n",
    "\\end{equation}\n",
    "\n",
    "def num_close(c):\n",
    "minc = argmin([dist(z,y) for y in data for z in c],axis = 1)\n",
    "return [sum(minc == c) for z in c]\n",
    "\n",
    "kmeans(c,cluster_number = k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draft of unit test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Item cost function - non_negativity\n",
    "* Item cost function - if c has more points cost should be smaller\n",
    "* Item cost function - if c has all points cost should be 0\n",
    "* Item probability in sampling is non negative\n",
    "* Item sum of probability in sampling is l (oversampling factor)\n",
    "* Item point in C of probability in sampling is 0\n",
    "* Item find number of closet points function - non negative integer\n",
    "* Item sum of the weight from weight function should be one\n",
    "* Item the weight from weight function should be non-negative\n",
    "* Item total levels of labels of KmeansParallel function should be the same as the number of cluster we want\n",
    "* Item number of labels should be equal to the number of data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization - parallel implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the initialization of K-means||, we can use MapReduce model of computation, especially for step 4 and step 7. As to step 4, when we sample each point in the data set in each iteration, we can assign each mapper to sample independently and combine the result. For step 7, we can also assign each mapper to calculate the number of points in data set closer to $c_i$ than any other  potential centers in $C$. In my code, I can use MapReduce model twice for step 7. Basically, I can assign each mapper to find the closest center for a data point independently and then assign each mapper to calculate how many data point is closer to the specific center than other centers independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate data which is meaningful for clustering, I simulate some data from a mixture of three bivariate normal distribution.\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\begin{pmatrix}x_{1}\\\\\n",
    "x_{2}\n",
    "\\end{pmatrix} & \\sim & N\\left[\\left(\\begin{array}{c}\n",
    "3\\\\\n",
    "5\n",
    "\\end{array}\\right),\\left(\\begin{array}{ccc}\n",
    "1 & 0\\\\\n",
    "0 & 2\n",
    "\\end{array}\\right)\\right]\\\\\n",
    "\\begin{pmatrix}y_{1}\\\\\n",
    "y_{2}\n",
    "\\end{pmatrix} & \\sim & N\\left[\\left(\\begin{array}{c}\n",
    "-2\\\\\n",
    "3\n",
    "\\end{array}\\right),\\left(\\begin{array}{ccc}\n",
    "1 & -0.6\\\\\n",
    "-0.6 & 1\n",
    "\\end{array}\\right)\\right]\\\\\n",
    "\\begin{pmatrix}z_{1}\\\\\n",
    "z_{2}\n",
    "\\end{pmatrix} & \\sim & N\\left[\\left(\\begin{array}{c}\n",
    "-6\\\\\n",
    "-1\n",
    "\\end{array}\\right),\\left(\\begin{array}{ccc}\n",
    "3 & 0.3\\\\\n",
    "0.3 & 1\n",
    "\\end{array}\\right)\\right]\\\\\n",
    "\\end{eqnarray*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Algorithm comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate the efficiency of the k-means|| algorithm, I compare it with k-means++ and general k-means with simple random initialization. I will try using different sizes of data which are simulated in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
